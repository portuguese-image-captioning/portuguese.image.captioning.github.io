<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:17px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
        font-size: 30px;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
    table td, table td * {
        vertical-align: top;
    }
</style>

<html>
  <head>
	  <title>An Introductory Study about Image Captioning in Portuguese using Deep Learning</title>
      <meta property="og:image" content="https://phillipi.github.io/pix2pix/images/teaser_for_fb_v4.png"/>
      <meta property="og:title" content="Image-to-Image Translation with Conditional Adversarial Networks" />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:36px">An Introductory Study about Image Captioning in Portuguese using Deep Learning</span><br><br>
	  		  <table align=center width=600px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:18px"><a href="https://github.com/BrunoHenriqueLiradosAnjos">Bruno H. L. dos Anjos</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:18px"><a href="https://github.com/ravibdf">Ravi B.D. Figueiredo</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:18px"><a href="https://github.com/jcrbsa">Richardson B. da S. Andrade</a></span>
		  		  		</center>
		  		  	  </td>
			  </table><br>
              <span style="font-size:18px"> Center of Informatics (Cin)</span><br>
			  <span style="font-size:18px"> Federal University of Pernambuco (UFPE)</span><br>
			  <span style="font-size:18px"> Recife, Pernambuco, Brazil</span><br>
              <span style="font-size:18px"></span><br><br>

	  		  <table align=center width=300px>
	  			  <tr>
	  	              <td align=center width=50px>
	  					<center>
							<img load="lazy" height="110px" src="./images/paper2.png">
							<br>
							
	  						<span style="font-size:24px"><a href='./paper/paper.pdf'>[Paper]</a></span>
		  		  		</center>
		  		  	  </td>

<!-- 					<td align=center width=50px>
							<center>
								
								<span style="font-size:24px"><a href='./paper/presentation.pdf'>[Presentation]</a></span>
							  </center>
					</td> -->
	  	              <td align=center width=50px>
	  					<center>
							<img load="lazy" height="110px" src="./images/git.png">
	  						<span style="font-size:24px"><a href='https://github.com/portuguese-image-captioning'>[GitHub]</a></span>
		  		  		</center>
		  		  	  </td>

						  <td align=center width=50px>
						  <center>
							<img load="lazy" height="110px" src="./images/bibtex2.png">
							<br>
		
							<span style="font-size:24px">
							  <a href="./bib/cite.txt" target="_blank">[Bibtex]</a>
							</span>
						  </center>
						</td>
						<td align=center width=50px>
						<center>
							<i class="fa fa-graduation-cap m-auto text-primary"></i>
						</center>
						</td>

			  </table>
          </center>

<!--   		  <br><br>
		  <hr> -->

  		  <br>
  		  <table align=center width=850px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<a href="./images/results.png"><img class="" src = "./images/results.png" height="360px"></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
                      <center>
  	                	<span style="font-size:14px"><i>Example results on portuguese image captioning.</i>
                      <center>
  	              </td>

  		  </table>




  		  <table align=center width=850px>
	  		  <center><h1>Abstract</h1></center>
	  		  <tr>
	  		  	<td>
					Image caption is the field of study that generates sentences describing the content of a given image. Many solutions has been proposed recently, the majority in English. However, Portuguese image captioning works has been lacking, with at the moment of this article, none papers has been published. Our purpose is to create an image caption dataset in Portuguese language using the MSCOCO as base. Also, test some deep neural networks to generate sentences in Portuguese. The results were validated using BLEU metric.
	  		    </td>
	  		  </tr>
			</table>
				
  		  <br><br>
		  <hr>


	  <!-- NETWORK ARCHITECTURE, TRY THE MODEL -->
 		<center><h1>Try our code</h1></center>

  		  <table align=center width=800px>
			  <tr><center>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
<!-- 				<div style="width:600px; text-align:left">
					<span style="font-size:28px">&nbsp;Recommended version: <a href='https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix '>[PyTorch]</a></span><br>
					<span style="font-size:28px">&nbsp;Original code: <a href='https://github.com/phillipi/pix2pix'>[Torch]</a></span>
				</div>
                    <br>
					 -->
                    <div style="width:800px; text-align:left">
                    Parts of our code:<br>
					<a href="https://github.com/portuguese-image-captioning/vgg16-lstm">VGG-16 + LSTM</a>  (implementation by <a href="https://github.com/jcrbsa">Richardson Bruno da Silva Andrade</a>)<br>
                    <a href="https://github.com/portuguese-image-captioning/efficientnet-transformer">EfficientNet + Transformer</a> (implementation by <a href="https://github.com/BrunoHenriqueLiradosAnjos">Bruno H. L. dos Anjos</a>)<br>
                    <a href="https://github.com/portuguese-image-captioning/clip-gpt2">CLIP + GPT2</a> (implementation by <a href="https://github.com/ravibdf">Ravi B.D. Figueiredo</a>)<br>
<!--                     <a href='https://github.com/pfnet-research/chainer-pix2pix'>[Chainer]<a/> (implementation by <a href="https://github.com/pfnet-research">pfnet-research</a>)<br>
                    <a href='https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/pix2pix'>[Keras]</a> (implementation by <a href="https://github.com/tdeboissiere">Thibault de Boissiere</a>)<br>
								<a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/Pix2pix-Photo-to-Street-Map-Translation">[Wolfram Cloud]</a> (implementation by Wolfram team)	 -->	
                </div>

			  <br>
			  </center></tr>
		  </table>

<!-- <a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2016_colorization/files/demo_v0/colorization_release_v0.caffemodel">[Model 129MB]</span> -->

      	  <br>
		  <hr>

			<center><h1>Paper</h1></center>
			<table align="center" width="700" px="">
			          <tbody><tr>
			          <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./images/paper_pdf_thumb.png"></a></td>
			          <td><br><br><br><span style="font-size:12pt">Bruno H. L. dos Anjos, Ravi B.D. Figueiredo, Richardson Bruno da Silva Andrade</span><br>
			          <b><span style="font-size:12pt">An Introductory Study about Image Captioning in Portuguese using Deep Learning</span></b><br>
			          <span style="font-size:12pt">Undefined, 2022 (<a href="./paper/paper.pdf">Paper</a>)</span>
			          </td>
								</tr>
					</table>
								<br>
          <table align="center" width="600px">
            <tbody>
              <tr>
                <td>
                  <center>
					<img load="lazy" height="110px" src="./images/bibtex2.png">
					<br>

                    <span style="font-size:22px">
                      <a href="./bib/cite.txt" target="_blank">[bibtex]</a>
                    </span>
                  </center>
                </td>
              </tr>
            </tbody>
          </table>
			
		  <br><br>
		</tr>
		  <center><h1>Approach Overview</h1></center>
          <hr>
          	  <table align=center width=1100px>
          		  <tr>
                        <td width=400px>
               					<left>
                          <center><a href="#"><img width=400px src="./images/overview.png"/></a></center>
						  <table align=center width=850px>
							<tr>
								<td>
									This work is divided into two parts. First, for the lack of dataset in Portuguese, the COCO dataset is translated using the Google Translation API the is integrated into the . The translation is made sentence by sentence to keep the context of individuals' captions. The COCO dataset is formatted to competitions, i.e., the image test does not have captions, then the validation set is used like the test set, and 10% of the train set is separated for the validation task. In the second part is described the three architectures used to generate image captions in Portuguese, VGG16 + LSTM, EfficientNet + Transformer Encoder-Decoder e CLIP + GPT2, and shows results with BLEU metrics.
							  </td>
							</tr>
						</table>
							
						 
                  </td>
              </tr>
          </table>
          <br>


		  <hr>
		  <center><h1>Architectures </h1></center>
		  <table align=center width=1100px>
			<tr>
				<td width=400px>
						   <left>
		
  
				  <table align=center width=1100px>
							  <tr>

								
							<td width=300px>
								<center>
									<span style="font-size:22px"><a href='https://nono.ma/suggestive-drawing'>VGG16 + LSTM</a></span><br>
									<a href="https://nono.ma/suggestive-drawing"><img src="./images/vgg16.png" width = "250px"></a><br>
								<div style="width:250px; text-align:left; font-size:14px"><a href="https://nono.ma/">VGG16+LSTM</a> This deep architecture is composed by encoder and decoder. Commonly, the encoder utilizes Convolution Neural Network (CNN) to extract features and the decoder uses some variation of Recurrent Neural Networks (RNN) for sequence word generation.</div></center>
							</td>
						  <td width=300px>
							<center>
								<span style="font-size:22px"><a href='https://www.nvidia.com/en-us/deep-learning-ai/ai-art-gallery/artists/scott-eaton/'>Effficient + Transformer</a></span><br>
								<a href="https://www.nvidia.com/en-us/deep-learning-ai/ai-art-gallery/artists/scott-eaton/"><img src="./images/effcient_transform.png" width = "500px"></a><br>
							<div style="width:250px; text-align:left; font-size:14px"><a href="http://www.scott-eaton.com/">Effficient + Transformer</a> In this architecture is used EfficientNet to extract the features from images, and transformers to encode the features and decode the text. The objective of EfficientNet is to increase the scalability of the net based on the input, i.e., the number of layers and filters depends on the size of the image to better extract features of the image..</div></center>
						  </td>
  
						  <td width=300px>
							<center>
								<span style="font-size:22px"><a href='https://vimeo.com/260612034'>Clip + GPT-2</a></span><br>
								<a href="https://vimeo.com/260612034"><img src="./images/clip_gpt2.png" width = "500px"></a><br>
							<div style="width:250px; text-align:left; font-size:14px"><a href="http://www.memo.tv/">CLIP+GPT2</a> used pix2pix to create the very compelling music video linked above, in which common household items, like a powercord, are moved around in a pantomine of crashing waves and blooming flowers. Then a pix2pix-based model translates the pantomine into renderings of the imagined objects.</div></center>
						  </td>
  
  
				</tr>
										  
			
  </table>
	<br>

<!-- 
          <hr>
	 		<center><h1>Expository articles and videos</h1></center>
     		  <br>
     		  <table align=center width=1100px>
     			  <tr>
     	              <td width=300px>
     					<center>
     						<span style="font-size:22px"><a href="https://www.youtube.com/embed/u7kQ5lNfUfg">Two-minute Papers</a></span><br>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/u7kQ5lNfUfg" frameborder="0" allowfullscreen></iframe>
     					    <div style="width:560px; text-align:left; font-size:14px">Karoly Zsolnai-Feher made the above as part of his very cool <a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">"Two-minute papers" series</a>.</div>
                        </center>
     	              </td>

     	              <td width=300px>
     					<center>
     						<span style="font-size:22px"><a href='https://affinelayer.com/pix2pix/'>Affinelayer blog post</a></span><br>
     	                	<a href="https://affinelayer.com/pix2pix/"><img src="./images/hesse_blog_post.png" width = "270px"></a><br>
     					<div style="width:270px; text-align:left; font-size:14px">Great explanation by Christopher Hesse, also documenting his <a href="https://github.com/affinelayer/pix2pix-tensorflow">tensorflow port</a> of our code.</div></center>
     	              </td>

                   </tr>
     		  </table>
		  <br><br> -->

		  <center><h1>Evaluation</h1></center>
          <hr>
		  <center><h1>Qualitative Results</h1></center>

		  <table align=center width=1100px>
			<tr>
				<td width=400px>
						   <left>
				  <center><a href="#"><img width=400px src="./images/cat_keyboard.jpg"/></a></center>
				  <br>
				  <table align=center width=850px>
					<tr  align=center ><td> <b>Portuguese</b> </td> <td> <b>English</b></td> </tr>
					<tr>
						<td>
							<ul>
								<li>Um gato laranja está dormindo em um teclado. </li>
									<li>Um close-up de um gato sentado ao lado do teclado. </li>
										<li>Um gato dormindo com é ouvir descansando em um teclado. </li>
											<li>Um gato com a cabeça apoiada em um teclado.</li>
						</ul>
					  </td>

					  <td>
						<ul>
							<li>An orange cat is sleeping on a keyboard.</li>
							<li>A close-up of a cat sitting next to the keyboard.</li>
							<li>A sleeping cat is hear resting on a keyboard.</li>
							<li>A cat with its head resting on a keyboard.</li>

						</ul>

					  </td>
					</tr>
				</table>
			<br>
		  <center><h1>Quantitative Results</h1></center>
          	  <table align=center width=1100px>
          		  <tr>
                        <td width=400px>
               					<left>
                          <center><a href="#"><img width=400px src="./images/result_metrics.png"/></a></center>
						  <br>
						  <table align=center width=850px>
							<tr>
								<td>
									In this work we use metrics BLEU 1, 2, 3 and 4 for quantitatie analysis, the results is presented in I. The CLIP + GPT2 architectures presents the best result in all cases, even showing a 64.56% of improvement. The VGG16+LSTM show the worst results in all cases. This can be explained by that the transformers model shows a superior performance in comparison with LSTM
							  </td>
							</tr>
						</table>
							
						 
                  </td>
              </tr>
          </table>

          <br>
		  <hr>


  		  <a name="bw_legacy"></a>
  		  <center><h1>Experiments</h1></center>
          Here we show comprehensive results from each experiment in our paper. Please see the paper for details on these experiments.<br>
          <br>

          <table align=center width=600px>

        <tr>
        <td valign="top">
  		  <b>Effect of Datasets</b><br>
          <a href="#">MS-Coco Dataset</a><br>
          <a href="#">Portuguse MS-Coco Dataset</a><br>
          <br>

  		  <b>Effect of architecture</b><br>
          <a href="#">VGG-16</a><br>
		  <a href="#">Autoencoder</a><br>
		  <a href="#">LSTM</a><br>
		  <a href="#">Transformers</a><br>
		  <a href="#">EfficientNet</a><br>
		  <a href="#">Clip</a><br>
		  <a href="#">GPT-2</a><br>
          <br>

      </td>
      <td valign="top">
		<b>Metrics</b><br>
		<a href="#">BLUE-1</a><br>
		<a href="#">BLUE-2</a><br>
		<a href="#">BLUE-3</a><br>
		<a href="#">BLUE-4</a><br>
		<br>
  		  <b>Additional results</b><br>
          <a href="#">Additional Result 1</a><br>
          <a href="#">Additional Result 2</a><br>
          <a href="#">Additional Result 3</a><br>
          <!--<a href="./images/colorization/index.html">Colorization</a><br>-->
        <br>

      </td>
      </tr>
          </table>


  	  	<hr>

	  <table align=center width=1100px>
		  <tr>
              <td width=400px>
     					<left>
      		  <center><h1>Use Cases for <a href="https://twitter.com/search?vertical=default&q=pix2pix&src=typd">#portuguese-image-captioning</a></h1></center>
              People have used our code for many creative applications, often posted on twitter with the hashtag #portuguese-image-captioning. 
			  <!-- Check them out <a href="https://twitter.com/search?vertical=default&q=pix2pix&src=typd">here</a>! Below we highlight just a few of the many:<br><br>
 -->
      		  <table align=center width=1100px>
							<tr>
      	              <td width=300px>
      					<center>
      						<span style="font-size:22px"><a href='#'>Art Description</a></span><br>
      	                	<a href="#"><img src="./images/art.png" width = "250px"></a><br>
      					<!-- <div style="width:250px; text-align:left; font-size:14px"><a href="#">Case 1</a> Todo.</div> --></center>
      	              </td>

      	              <td width=300px>
      					<center>
      						<span style="font-size:22px"><a href='#'>Scene Description </a></span><br>
      	                	<a href="#"><img src="./images/news.png" width = "250px"></a><br>
      					<!-- <div style="width:250px; text-align:left; font-size:14px"><a href="#"">Case 2</a> Todo.</div> --></center>
      	              </td>

      	              <td width=300px>
      					<center>
      						<span style="font-size:22px"><a href='#'>Medical Report</a></span><br>
      	                	<a href="#"><img src="./images/medical_report.png" width = "250px"></a><br>
      					<!-- <div style="width:250px; text-align:left; font-size:14px"><a href="#">Case3</a> Todo.</div> --></center>
      	              </td>

              </tr>
										
   <!--    			  <tr>
      	              <td width=300px>
												<br>
      					<center>
      						<span style="font-size:22px"><a href='https://twitter.com/search?vertical=default&q=edges2cats&src=typd'>#edges2cats</a></span><br>
      	                	<a href="https://twitter.com/search?vertical=default&q=edges2cats&src=typd"><img src="./images/cats_example.jpg" width = "250px"></a><br>
      					<div style="width:250px; text-align:left; font-size:14px"><a href="https://twitter.com/christophrhesse?lang=en">Christopher Hesse</a> trained our model on converting edge maps to photos of cats, and included this in his <a href="https://affinelayer.com/pixsrv/">interactive demo</a>. Apparently, this is what the Internet wanted most, and #edges2cats briefly <a href="https://www.youtube.com/watch?v=vbuE5CLRCDY">went viral</a>. The above cats were designed by Vitaly Vidmirov (<a href="https://twitter.com/vvid/status/834976420942204933">@vvid</a>).</div></center>
      	              </td>

      	              <td width=300px>
      					<center>
      						<span style="font-size:22px"><a href='https://www.youtube.com/watch?v=af_9LXhcebY'>Alternative Face</a></span><br>
      	                	<a href="https://www.youtube.com/watch?v=af_9LXhcebY"><img src="./images/hardy_conway.jpg" width = "250px"></a><br>
      					<div style="width:250px; text-align:left; font-size:14px"><a href="https://quasimondo.com/">Mario Klingemann</a> used our code to translate the appearance of French singer Francoise Hardy onto Kellyanne Conway's infamous "alternative facts" interview. Interesting articles about it can be read <a href="http://nymag.com/selectall/2017/03/pix2pix-cat-drawing-tool-is-ai-at-its-best.html">here</a> and <a href="http://www.alphr.com/art/1005324/alternative-face-the-machine-that-puts-kellyanne-conway-s-words-into-a-french-singer-s">here</a>.</div></center>
      	              </td>

      	              <td width=300px>
      					<center>
      						<span style="font-size:22px"><a href='https://twitter.com/brannondorsey/status/808461108881268736'>Person-to-Person</a></span><br>
      	                	<a href="https://twitter.com/brannondorsey/status/808461108881268736"><img src="./images/dorsey_kurzweil.jpg" width = "250px"></a><br>
      					<div style="width:250px; text-align:left; font-size:14px"><a href="https://brannondorsey.com/">Brannon Dorsey</a> recorded himself mimicking frames from a video of Ray Kurzweil giving a talk. He then used this data to train a Dorsey&rarr;Kurzweil translator, allowing him to become a kind of puppeter in control of Kurzweil's appearance.</div></center>
      	              </td>

                    </tr>

                    <tr>
        	              <td width=300px>
                              <br>
        					<center>
        						<span style="font-size:22px"><a href='https://twitter.com/bgondouin/status/818571935529377792'>Interactive Anime</a></span><br>
        	                	<a href="https://twitter.com/bgondouin/status/818571935529377792"><img src="./images/iPokemon.jpg" width = "250px"></a><br>
        					<div style="width:250px; text-align:left; font-size:14px"><a href="https://bgon.github.io/">Bertrand Gondouin</a> trained our method to translate sketches&rarr;Pokemon, resulting in an interactive drawing tool.</div></center>
        	              </td>

        	              <td width=300px>
                              <br>
        					<center>
        						<span style="font-size:22px"><a href='http://www.k4ai.com/imageops/index.html'>Background masking</a></span><br>
        	                	<a href="http://www.k4ai.com/imageops/index.html"><img src="./images/background_masking.jpg" width = "250px"></a><br>
        					<div style="width:250px; text-align:left; font-size:14px"><a href="https://twitter.com/kaihuchen?lang=en">Kaihu Chen</a> performed <a href="http://www.k4ai.com/tag/gan/index.html">a number of interesting experiments</a> using our method, including getting it to mask out the background of a portrait as shown above.</div></center>
        	              </td>

      	              <td width=300px>
                          <br>
      					<center>
      						<span style="font-size:22px"><a href='http://colormind.io/blog/'>Color palette completion</a></span><br>
      	                	<a href="http://colormind.io/blog/"><img src="./images/color_palettes.jpg" width = "250px"></a><br>
      					<div style="width:250px; text-align:left; font-size:14px"><a href="http://colormind.io/blog/">Colormind</a> adapted our code to predict a complete 5-color palette given a subset of the palette as input. This application stretches the definition of what counts as "image-to-image translation" in an exciting way: if you can visualize your input/output data as images, then image-to-image methods are applicable! (not that this is necessarily the best choice of representation, just one to think about.)</div></center>
      	              </td>

                    </tr>
      		  </table> -->
              <br>
        </td>
    </tr>
</table>

        <hr>


 		  <a name="related_work"></a>
 		  <table align=center width=1100px>
 			  <tr>
 	              <td width=400px>
 					<left>
  		  <center><h1>Recent Related Work</h1></center>

			<br> <br>
			M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, <b>A comprehensive survey of deep learning for image captioning</b>, ACM Computing Surveys (CSUR), vol. 51, pp. 1 – 36, 2019.
			<br> <br>
			M. Bartosiewicz, I. Krupinska, M. Bany, A. Konieczna, M. Ostrowski, M. Zalewski, and M. Iwanowski, <b>Generating image captions in polish experimental study</b>, in 2021 14th International Conference on Human System Interaction (HSI), pp. 1–6, 2021.
			 G. O. dos Santos, E. L. Colombini, and S. Avila, pracegover: A large dataset for image captioning in portuguese, Data, vol. 7, no. 2, 2022.
			<br> <br>
			 D. C. Caterina Masotti and R. Basili, <b>Deep learning for automatic imagecaptioning in poor training conditions</b>, in Emerging Topics at the Fouth
			Italian Conference on Computational Linguistics, 2018.
			<br> <br>
			 C. Masotti, D. Croce, and R. Basili, <b>Deep learning for automatic image captioning in poor training conditions</b>, IJCoL. Italian Journal of Computational Linguistics, vol. 4, no. 4-1, pp. 43-55, 2018.
			<br> <br>
			A. Wroblewska, <b>Polish corpus of annotated descriptions of images</b>,in Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), (Miyazaki, Japan), European Language Resources Association (ELRA), May 2018.
			<br> <br>
			 S. K. Mishra, R. Dhir, S. Saha, P. Bhattacharyya, and A. K. Singh,<b>Image captioning in hindi language using transformer networks</b>,Computers Electrical Engineering, vol. 92, p. 107114, 2021.	
			<br> <br>
			H. Wang, Y. Zhang, and X. Yu, <b>An overview of image caption generation methods</b>, Computational intelligence and neuroscience, vol. 2020,
			2020.
			<br> <br>
			M. Tan and Q. V. Le, <b>Efficientnet: Rethinking model scaling for convolutional neural networks</b>, CoRR, vol. abs/1905.11946, 2019.
			<br> <br>
			A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,<b> Attention is all you need</b>, CoRR, vol. abs/1706.03762, 2017.
			<br> <br>
			R. Mokady, A. Hertz, and A. H. Bermano, <b>Clipcap: Clip prefix forimage captioning</b>, 2021.
			<br> <br>
			 A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,<b>Language models are unsupervised multitask learners</b>,2018.
			
  		  <br><br>
			</td>
		 </tr>

	 </table>

	  <br>
	  <hr>
	  <br>


  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>

			We thank João Paulo, Thiago Vinuto, and Nickson Arraes for helpful discussions. 
			Thanks to authors of works shared datasets and code to replication.
			This work was supported in part by UFPE via Cin, Samsung, SiDi, FADE.
			<!-- The hardware donations by Samsung. --> 
<!-- 			Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL or the U.S. Government.-->
			<center>
				<a href="./images/samsung_ufpe_fade_logo.png"><img class="" src = "./images/samsung_ufpe_fade_logo.png" width="500px"></img></href></a>
				<br>
			</center>			
			</left>
		</td>
			 </tr>
		</table>

		<br><br>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24665197-6', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>
